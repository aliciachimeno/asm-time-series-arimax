---
title: "London housing price forecast"
author: "Al√≠cia Chimeno and Silvia Ferrer"
output: "html_document"
date: "2024-12-15"
---
## Data 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

```{r}
library(readxl)
avg_price <- read_excel("/Users/ali/Documents/GitHub/asm-time-series-arimax/data/UK-house-price-index.xlsx", sheet = 3)
```

```{r}
source("validation.R")
```

## 0. Pre-processing
```{r}
london_price<-avg_price[2:dim(avg_price)[1],c("...1","LONDON")] # select data
london_price$LONDON <- as.numeric(london_price$LONDON) # transform  into numerical data
```

## 1. Identification

### a) Determine the needed transformations to make the series stationary. Justify the transformations carried out using graphical and numerical results.

```{r}
serie <- ts(london_price$LONDON, start = c(1995, 1), freq = 12)
plot(serie,main="London housing prices")
abline(v=1995:2024,col=4,lty=3)
```

We shall diagnose if the series is stationary. And if it is not, try to transform the series into a stationary one. 

#### Is the variance constant?
To diagnose the non-constant variance, we will check 2 plots: 

- Boxplot 
```{r}
boxplot(serie~floor(time(serie)), xlab='time (years)')
```

 - Mean/Var plot
```{r,warning=F}
mserie <- matrix(serie, ncol = 12, byrow = TRUE) 
m <-apply(mserie, 2, mean) # mean of each column (year)
v <-apply(mserie, 2, var) 
plot(v ~ m, xlab='mean', ylab='variance')
abline(lm(v ~ m), col=2, lty=3)
text(m,v,1995:2024)
```

The first plot we can observe that the variance is not consistent for all years. There are bigger and smaller boxes during the years. In the second plot, we can see that for higher values of the mean we have higher values of the variance. Therefore we shall consider transforming the scale. We can check by the BoxCox method if the logarithm would be a good option. 
```{r}
library(forecast)
BoxCox.lambda(serie+1)
```
We have that lambda is close to 0, therefore a logarithmic transformation seems reasonable. 
```{r}
lnserie=log(serie)
plot(lnserie)
```

And check the same plots whether it improved or not. 
```{r, warning=F}
#boxplot
boxplot(lnserie~floor(time(lnserie)), xlab='time (years)')

#mean/var plot
mserie <- matrix(lnserie, ncol = 12, byrow = TRUE) 
m <-apply(mserie, 2, mean) # mean of each column (year)
v <-apply(mserie, 2, var) 
plot(v ~ m, xlab='mean', ylab='variance')
abline(lm(v ~ m), col=2, lty=3)
text(m,v,1995:2024)
```

```{r}
# comparison boxplot between series
par(mfrow=c(1,2))
boxplot(serie~floor(time(lnserie)), xlab='time (years)') 
boxplot(lnserie~floor(time(lnserie)), xlab='time (years)')
```

The boxes seem to became smaller. 
#### Is there a Seasonal Pattern?

```{r}
monthplot(lnserie)
```

There is no variations across the months, therefore seems to not have a seasonal pattern. 
```{r}
ts.plot(matrix(lnserie,nrow=12),col=1:8)
```

Does not seem to have seasonal monthly patterns. 
We will plot the decomposition of additive time series to see more insights.

```{r}
decomposed <- decompose(lnserie,type = "additive")
plot(decomposed)
```
We can see that there is a seasonal patter, later on we will explore more. 

#### Is the mean constant? 
There is kind of an almost-linear trend. We will apply one regular difference trying to make the mean constant:
```{r}
d1lnserie=diff(lnserie)
plot(d1lnserie)
#abline(h=mean(lnserie))
abline(h=mean(d1lnserie),col=2)

```


The mean from the `lnserie` is not constant.
```{r}
mean(lnserie)
```
And the mean from `d1lnserie` is close to 0:

```{r}
mean(d1lnserie)
```

We shall check for over-differentiation: Take an extra differentiation and compare the variances.
```{r}
d1d1lnserie=diff(d1lnserie)
plot(d1d1lnserie)
abline(h=0)
abline(h=mean(d1d1lnserie),col=2)
```

```{r}
var(lnserie)
var(d1lnserie)
var(d1d1lnserie)

mean(lnserie)
mean(d1lnserie)
mean(d1d1lnserie)
```
Although the mean is closer to 0, the variance increase when we take an extra regular difference. Therefore we do not need an extra differentiation. 

Final transformation for the London housing series: $W_t=(1-B)log(serie)$ 

### b) Analyze the ACF and PACFof the stationary series to identify at least two plausible models. Reason about what features of the correlograms you use to identify these models.


The `d1lnserie` ACF is the following:

```{r}
acf(d1lnserie,ylim=c(-1,1),lag.max=12*5,lwd=2,col=c(2,rep(1,11)), main ="ACF(d1d1lnserie)")
acf(d1lnserie,ylim=c(-1,1),lag.max=12*25,lwd=2,col=c(2,rep(1,11)), main ="ACF(d1lnserie)")

```
The ACF plot shows a clear spike at lag 12, but more precisely if you see a recurring cycle every 12 months, we likely have a yearly seasonality in the data.


The ACF appears to tail off slowly, rather than cutting off abruptly at one lag, which might indicate that the series has autoregressive (AR) behavior. We can see decreasing patterns (infinity lags not null). 
In this context, makes sense that the price of the house deppends on the past price. 


And the PACF is the following: 
```{r}
pacf(d1lnserie,ylim=c(-1,1),col=c(rep(1,11),2),lwd=2,lag.max=12*5,main ="PACF(d1lnserie)")
pacf(d1lnserie,ylim=c(-1,1),col=c(rep(1,11),2),lwd=2,lag.max=12*25,main ="PACF(d1lnserie)")

```


MODELS
- ARIMA(1,1,0)(1,0,1)[12] 
- ARIMA(1,1,1)(1,0,1)[12] 
- 

## 2. Estimation

### a) Use R to estimate the identified models.
With the plot we observed some characteristics of the time series, but we will check which is the best model numerically by trying to optimize the metrics: AIC (minimize), loglike (maximize) and BIC (minimize). 

Manually we can see that :

```{r}
model1 <- arima(lnserie, order = c(1, 1, 0), seasonal = list(order = c(1, 0, 1), period = 12))
summary(model1)

model2 <- arima(lnserie, order = c(1, 1, 1), seasonal = list(order = c(1, 0, 1), period = 12))
summary(model2)

model3 <- arima(lnserie, order = c(2,2, 1), seasonal = list(order = c(1, 0, 1), period = 12))
summary(model3)

Arima(lnserie, order = c(2,2, 1), seasonal = list(order = c(1, 0, 1), period = 12))
```


## 3. Validation

```{r}
validation(model1)
validation(model2)
validation(model3)
```

### a) Perform the complete analysis of residuals, justifying all assumptions made. Use the corresponding tests and graphical results. 


### b) Include analysis of the expressions of the AR and MA infinite models, discuss if they are causal and/or invertible and report some adequacy measures.

##### Model 1.
**Expressions analysis**:
**Causality and inveritbility**: The model is both causal and invertible. This can be confirmed analytically by observing that the modulus of the AR  characteristic polynomial roots and the modulus of the MA characteristic polynomial roots are greater than 1. Additionally, the model is graphically confirmed as causal and invertible, since the inverse roots of both the AR and MA polynomials lie inside the unit circle.

##### Model 2. 
**Causality and inveritbility**: The model is both causal and invertible. Same reason as for model 1. 

##### Model 3: 
**Causality and inveritbility**: The model is both causal and invertible. Same reason as for model 1. 


### c) Check the stability of the proposed models and evaluate their capability of prediction, reserving the last 12 observations. 

We have seen that the model with the best AIC, loglike and BIC is ... Model. But to compare we will use RME, RSM forcasting error 

```{r}
## split train
train_data <- lnserie[1:(length(lnserie) - 12)]
test_data <- lnserie[(length(lnserie) - 11):length(lnserie)]
## model 1
model1 <- arima(train_data, order = c(1, 1, 0), seasonal = list(order = c(1, 0, 1), period = 12))
model2 <- arima(train_data, order = c(1, 1, 1), seasonal = list(order = c(1, 0, 1), period = 12))
model3 <- arima(train_data, order = c(2,2, 1), seasonal = list(order = c(1, 0, 1), period = 12))

forecast_values1 <- forecast(model1, h = 12)
forecast_values2 <- forecast(model2, h = 12)
forecast_values3 <- forecast(model3, h = 12)

accuracy(forecast_values1, test_data)
accuracy(forecast_values2, test_data)
accuracy(forecast_values3, test_data)

```

### d) Select the best model for forecasting. 

```{r}
model3 <- arima(lnserie, order = c(2, 2, 1), seasonal = list(order = c(1, 0, 1), period = 12))
best_model<-model3
```


## 4. Predictions

### a) Obtain long term forecasts for the twelve months following the last observation available; provide also confidence intervals. 



```{r}
forecast_results <- forecast(best_model, h = 12)
plot(forecast_results)
print(forecast_results)
```

```{r}
# Subset the last 5 years (80% window) of historical data
historical_data <- window(lnserie, start = time(lnserie)[length(time(lnserie)) - (5 * 12) + 1])

# Apply the exponential transformation to revert to the original scale
forecast_df <- data.frame(
  Time = c(time(historical_data), time(forecast_results$mean)),
  Value = c(exp(as.numeric(historical_data)), exp(as.numeric(forecast_results$mean))),
  Type = c(rep("Historical", length(historical_data)), rep("Forecast", length(forecast_results$mean))),
  Lower80 = c(rep(NA, length(historical_data)), exp(as.numeric(forecast_results$lower[, 1]))),  # 80% CI lower
  Upper80 = c(rep(NA, length(historical_data)), exp(as.numeric(forecast_results$upper[, 1]))),  # 80% CI upper
  Lower95 = c(rep(NA, length(historical_data)), exp(as.numeric(forecast_results$lower[, 2]))),  # 95% CI lower
  Upper95 = c(rep(NA, length(historical_data)), exp(as.numeric(forecast_results$upper[, 2])))   # 95% CI upper
)

library(ggplot2)

ggplot(forecast_df, aes(x = Time, y = Value, color = Type)) +
  geom_line(size = 1) +  # Line for historical and forecast data
  # 80% confidence interval ribbon
  geom_ribbon(data = subset(forecast_df, Type == "Forecast"), 
              aes(ymin = Lower80, ymax = Upper80), fill = "lightgreen", alpha = 0.3) +
  # 95% confidence interval ribbon
  geom_ribbon(data = subset(forecast_df, Type == "Forecast"), 
              aes(ymin = Lower95, ymax = Upper95), fill = "lightblue", alpha = 0.4) +
  labs(title = "Time Series with 1-Year Forecast and Confidence Intervals",
       x = "Time",
       y = "Housing Price (Original Scale)",
       color = "Data Type") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_continuous(breaks = seq(floor(min(forecast_df$Time)), ceiling(max(forecast_df$Time)), by = 1)) +
  scale_color_manual(values = c("Historical" = "black", "Forecast" = "blue"))


```

